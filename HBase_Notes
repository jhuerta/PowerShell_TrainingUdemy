-------------------------------------------------------------------------------------------------------------------------
HDFS: 
    Distributed file system
    Does not support fast access to individual records -> Use Hbase
    Only sequencial access to data -> Use hbase for random access
-------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------
HADOOP TUTORIAL FOR BEGINNERS | HADOOP ECOSYSTEM EXPLAINED IN 20 MIN! - FRANK KANE [9/10]
https://www.youtube.com/watch?v=DCaiZq3aBSc
    Haddop: Software Platform or ecosystem of applications, running on multiple computer (commodity) and using distributing storage and processing
    Horizontal scaling is linear
    It is not anymore for batch processing (Like HBase)
    World Of Hadoop
        Core Hadoop Ecosystem: Apps built on top directly of the Hadoop platform
            MapReduce, Yarn and HDFS is specifically Hadoop itself. Other apps like Hive, HBAse, Spark, Zookeper are apps built to solve specific problems
            YARN: Resource Negotiator. System manages the resources in the system cluster. It is where the processing start.
            MapReduce: Allows to process the data in the cluster (originally was included or integrated with YARN. Then it was splitted, allowing to create other tools)
            Pig: On top of MapReduce, SQL style syntax. 
            Hive: On top of MapReduce, SQL queries itself (or extremely similar)
            Apache Ambari: Sits on top of all the Hadoop Ecosyste, providing a huge overview of the running hadoop system
            Spark: At the level of MapReduce. Running queries on the data. Python/Scala/Java. Extremely powerful
            HBase: Expose your HDFS data, as a non sql database
            OOzie: For scheduling tasks and multiple jobs
            Zookeper: Coordinate all that is going on with the clusters, monitoring the up state of the nodes
            Data Ingestion: Apps to insert data into your HDFS: Sqoop, Flume, Kafka
    External Data Storage:
        *SQL, Cassandra, MongoDB: If you want to expose your data in real time applications, like a web. You can add a layer of this RDBMS databae to expose your data
    Query Engines (on top of you Hadoop clusters)
        To Query your data
        Apache Drill to query your databases
-------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------
HBASE BOOK (Tutorials Point, HBase Haddop Data) [2/10]
HBase: Non relational Databaes built on top of Hadoop file system (HDFS)
    Real time read/write quick random access to HDFS
    To read, only through HBase. 
    To write, either HBase or directly to HDFS

HBase organization
    Table contains rows
    Rows contain family columns
    Family columns contains collection of columns
    Each column contain key,value pair

HBase will store data:
    Denormalized
    No schema, only column families
    Wide tables
    No transactions
-------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------
INTRODUCTION TO HBASE COMMAND LINE | HBASE SHELL COMMANDS | HBASE TUTORIAL [7/10]
https://www.youtube.com/watch?v=_T9-Hmp1mEY

HMaster deamon should be running
>hbase shell : To go inside hbase console
hbase>list --> List of tables in hbase
hbase>version --> Version of hbase
hbase>create 'table_name', 'column_family_name_1', 'column_family_name_2' 
hbase>disable 'table_name'
hbase>drop 'table_name' (should be first disabled)
hbase>put 'table_name','row_key','column_family:column_name', 'value' (column name may or may not exist, but the column family should exist)
        PUT is upsert: update if exists, or create if it does not.
hbase>scan 'table_name' --> To list the content of the table (there may show multiple records, notice the key as this what sets that is one unique record)
hbase>get 'table_name', 'row_key'
hbase>get 'table_name', 'row_key','column_family'
hbase>get 'table_name', 'row_key','column_family:column_name'
hbase>delete 'table_name', 'row_key','column_family:column:name'
hbase>alter 'table_name', 'delete' => 'column_family' (delete column family)
hbase>alter 'table_name', {NAME=>'column_family'} (create column family)
hbase>alter 'new_table_name', {NAME=>'column_family', VERSIONS=>3} (create new table)
hbase>count 'table_name' ==> Number of records in a table
hbase>scan 'table_name', {COLUMN:'column_family:column_name', VERSIONS=>3} (Get multiple NUMBER versions)
-------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------
NOSQL TUTORIAL FOR BEGINNERS | INTRODUCTION TO NOSQL DATABASES | NOSQL DATABASES TUTORIAL [7/10]
https://www.youtube.com/watch?v=2yQ9TGFpDuM
    Good for huge databases and scalability and with data is coming through at high velocity
    Non-Related tables and denormalized
    Horizontal scalability
    Each row of table has a key that retrieve the whole collection/record/document
    Each record/collection can have different key/values
    Unstractured data
    Types:
        Graph databases
        Document based
        Column oriented databases
        Wide Column Stores -> HBase
            Good for lookups
    Consistency Availability Partition Tolerance --> CAP Theorem: CHOOSE 2 for distributed databases
        Partition Tolerance: The failure in one machine, does not affect the other machines
        Availability: Like redundancy, make sure that there is always data (duplicated). Regardless the server goes down, the data is in other server
        Consistency: All the clients sees always the same data (with data duplicated this is not always possible)
        HBase (and most of NoSQL) has Consistency and Partition Tolerance

-------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------
HBASE TUTORIAL | INTRODUCTION TO HBASE | WHAT IS HBASE? | HBASE TUTORIAL FOR BEGINNERS [8/10]
https://www.youtube.com/watch?v=VRD775iqAko
    Hbase as a key/value pair database
    It is quite fast to access column families (as it skip the other column families)
    HBase Master(s): Administrative tasks
        Create/delete tables
        Handle region splitted
    HBase slaves (Region Servers):
        This are the Region Servers, that communicates with the HBase Masters
        A region server is 1GB by default
        Data is stored in region servers as HFiles (files)
        A region servers contains thousands of regions
        Data is splitted in regions by rows
        Components:
            HFile: 
                Contains the actual data on file, sorted as key/value pair
                The content cannot be updated directly (use memstore)
            Memstore:
                Writes are stored here, in RAM
                One memstore per column family in the hbase table
                When the Memstore gets full, the data is transferred to the HFile, and the memstore flushed (flush activity)
            WAL (Write Ahead Log):
                Keeps a log in file of the memstore (data and structure of MemStore), to prevent losses in case of failure of Region Server (RAM data will be lost)
                Used for recovery among machines
                It can be skipped, increasing the throughput of data written into HBase. Only if the system tolerates data lost in case of server failure.
            Block Cache
                One block cache per region server. Stored in RAM.
                It is used for read operations, it contains latest keys read
                If not found here, it searches in Memstore. Finally, in HFile
        Data is stored first in WAL, then in Memstore. And then, the client will be ack that written is done
    Zookeper:
        Perform the distribution coordination
        The region servers and the HMasters sends the heartbeat to the the Zookeper, so they are aware which ones are active
    Metatable
        The client request from the Zookeeper the address of the datatable
        It is accessed by the client directly, to find out easily which region server the key is located
        Contains the start key and end key of each region server (or region?)
-------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------
APACHE HBASE - JUST THE BASICS: https://www.youtube.com/watch?v=2Ci_QxJ1kiE [9/10]
HBase (by Jesse Anderson)
    NoSQL on top of HDFS. HDFS handles replication for reliability and durability (x3).
    Rest Interface allows HTTP access
    There is also a HBase Java API
    SQL Interfaces (non-native): 
        Using Hive to use SQL to access HBase.
        This interfaces are on top of HBase, as a layer that receives SQL and send/translate it to HBase
    It stores versions of the data! (with the timestamp)
    HBase only stores BYTES!!!
        It is up to the client to know that sort of data was stored, and then, convert it back to the primitive type.
    Dont architect HBase as a relational database.
        Dont think in HBase tables in terms of relations
        Think about how you will access the data for read and write. 
            Which data is related
            What is the fastest way to access the data
            Think of denormalization
            Eventhough the data will be stored multiple times, it will beneffict the access (read)
            Focus on READ
    ONLY ONE INDEX OR PRIMARY KEY PER ROW!
        No increment data
        It can hold multiple pieces of data (glued together)
    The design of the schemas, should focus on how the client will access the data
        Fewer, bigger denormalized tables
        Focus on bulk loading the data. Instead of looping in a series of data to insert, do it all at once
-------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------
AO DBA APACHE HADOOP HBASE (https://www.youtube.com/playlist?list=PLkp40uss1kSIVC891Bv2HZiFmTBdk3j9v) [7/10]

1. Introduction into Apache HBASE
    HBase is atomicity at row level
    Does not support table joins

2. Differences between Apache HBASE vs RDBMS
    Terabytes to petabyes
    Data layout: sparse, distributed, persistent and multidimensional sorted map
        (rows and columns in RDBMS)
    Supports millions of requests per second

3. Apache HBASE Table characteristics
    Usually 2 to 3 column families
    HBase Shell 
        Start: HBASE_HOME/bin/start-hbase.sh
        Stop: HBASE_HOME/bin/stop-hbase.sh
        Start shell: HBASE_HOME/bin/hbase shell

4. Working with the Apache HBASE client API
    HTable class in Java provides the CRUD operations: Put (add and update), Get, Delete
    checkAndPut() -> Atomic method that run a test to validate the data exist in the row before executing the put 

5. Working with the Apache HBASE Delete Command
    Batch Operations: 
        Combine multiple CRUD operations into one operation
        The order of execution is not known!!!

6. Working with the Apache HBASE SCAN Operation
    Release the scan object as soon as you are finished, as it quite expensive for the system (scanner.close)
    When scanning from/to rows, the start is inclusive, but to end is exclusive [start, end)
    The getScanner returns a ResultScanner. If you loop through the results, you will use next(). This will make a call to the server each time. In order to avoid many calls to the server, you can enable Scanner Caching (disabled by default) [read more]

7. HBASE Client API administration and Advanced features
    Part of the HBaseAdmin class
        Table operations: createTable (passing table description and column family description)
        Schema operations: addColumn, deleteColumn, modifyColumn

8. Working with Filters in Apache HBASE
    Use filter to get closer to the result you want. 
    There are existing or custom filters
    Three main type:
        CompareFilters: 
            Comparison Operators: LESS, EQUAL, NOT_EQUAL, GREATER, NO_OP, ...
            Comparators: BinaryComparator, SubstringComparator, BitComparator, ...
            Ex. RowFilter(OPERATOR, COMPARATOR ) (filter by the key)

9. Working with Dedicated Filters in Apache HBASE
    DedicatedFilters
    Filter Decorators: 
        SkipFilter: Remove records on the main filter based on second filter
        WhileMatchFilter: Stop filtering if a match is found
    FilterList: Apply multiple filters (AND or OR conditions with MUST_PASS_ONE for OR and MUST_PASS_ALL for AND)
    Custom filters

    Bloom Filters definied during the table creation!!!
        The scan will have to look to a subset of known HFiles (instead of all the files), so is faster
        It adds extra space
        Two types:
            Row
            Row ColumnFamily
    Counters:
        Columns that are defined as counters
        Math operations (increment) will be performed on those columns atomically

10. Working with Apache HBASE ImportTSV
    Built-in tool to import tab-separated values into HBase from the terminal

11. Available Clients for Apache HBASE
    HBase Shell
    Interactive clients (expects ersponse in real time
        HBase Java API
        REST (inclded by default)
            $HBASE_HOME/bin/hbase-daemon.sh start rest
            You can use Java API as the client: RemoteHTable, RemoteAdmin
        Thrift (needs to be installed)
    Batch Clients
        MapReduce (Java, Hive, Pig)
    Web-based UI

14. Apache HBASE and MapReduce integration
    HBase can be a source (origin) and/or sink (destination) for a MapReduce job
    HBase provides classes for each step of the MapReduce process: TableInputFormat, TableMapper, TableReducer, TableOutputFormat

-------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------
VIRTUAL MACHINES
Bitmani:
    http://localhost:8088/cluster
    Username/password: bitnami/bitnami
    SSH: 
        Enable: https://docs.bitnami.com/virtual-machine/faq/get-started/enable-ssh/
        Check Ip in console (ifconfig)

HDP Hortonworks:
    2.6.5: https://www.cloudera.com/downloads/hortonworks-sandbox/hdp.html
        Username/password: maria_dev/maria_dev
        Web ssh: http://localhost:4200/
        Terminal ssh: 2222
        HDP UI Dashboard: http://localhost:1080/splash.html
-------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------
HBASE WITH PLAYING CARDS
https://www.youtube.com/watch?v=9CREwiemvT0

    HBase split the table in rows, and store the group of rows into separate cluster nodes. Each group of rows is called a region.
    The data is stored in order. In addition HBase store index to know where to start looking in the table.
    Since the data is ordered, it is easy to find the missing index (if searching for a non existing index)
-------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------
HBASE JAVA API
http://www.corejavaguru.com/bigdata/hbase-tutorial/hbase-java-client-api-examples
https://www.baeldung.com/hbase
https://www.programcreek.com/java-api-examples/index.php?api=org.apache.hadoop.hbase.util.Bytes
-------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------
HBASE ON MAC
https://medium.com/@gnakan/getting-started-with-apache-hbase-31182755331
-------------------------------------------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------------------------------------------
QUESTIONS
    Do I need to install Hadoop/hbase in my local machine? even for learning? Or should I use the one in dev/test environment?
-------------------------------------------------------------------------------------------------------------------------
